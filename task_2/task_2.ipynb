{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heartbeat Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from src.features import feature_extraction\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\".\")\n",
    "\n",
    "PATH_TO_DATA = ROOT / \"data\"\n",
    "PATH_TO_X = PATH_TO_DATA / \"X_train.csv\"\n",
    "PATH_TO_Y = PATH_TO_DATA / \"y_train.csv\"\n",
    "\n",
    "PATH_TO_FEATURES = PATH_TO_DATA / \"features.csv\"\n",
    "ENCODING = \"utf-8\"\n",
    "COMPRESSION = None\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Reproducibility\n",
    "##################################################\n",
    "SEED = 3\n",
    "random.seed(SEED)\n",
    "RS_NUMPY = np.random.RandomState(SEED)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Others\n",
    "##################################################\n",
    "SAMPLING_FREQ = 300  # Hz\n",
    "feature_extract = lambda signal: feature_extraction(signal, SAMPLING_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = None\n",
    "\n",
    "# Check if the features were already computed.\n",
    "if PATH_TO_FEATURES.exists():\n",
    "    features = pd.read_csv(PATH_TO_FEATURES, index_col=0)\n",
    "else:\n",
    "    # Load raw ECG signals.\n",
    "    X = pd.read_csv(PATH_TO_X, index_col=0)\n",
    "\n",
    "    # Sanity checks.\n",
    "    # Check for non-empty DataFrames.\n",
    "    assert not X.empty, \"X is empty.\"\n",
    "\n",
    "    # Extract features from ECG signals.\n",
    "    # Use Dask to parallelize the computation.\n",
    "    columns = feature_extract(X.iloc[0]).index.to_list()\n",
    "    meta = {col: \"float64\" for col in columns}\n",
    "\n",
    "    dX = dd.from_pandas(X, npartitions=4)\n",
    "    features_dask = dX.map_partitions(\n",
    "        lambda df: df.apply(lambda x: feature_extract(x), axis=1),\n",
    "        meta=meta,\n",
    "    )\n",
    "    features = features_dask.compute()\n",
    "\n",
    "    # Save the computed features to a CSV file.\n",
    "    features.to_csv(PATH_TO_FEATURES, encoding=ENCODING, compression=COMPRESSION)\n",
    "\n",
    "\n",
    "assert features is not None, \"Error: `features` DataFrame incorrectly initialized.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Loading the Labels\n",
    "##################################################\n",
    "# Load labels.\n",
    "y = pd.read_csv(PATH_TO_Y, index_col=0)\n",
    "\n",
    "\n",
    "# Sanity checks.\n",
    "# Check for non-empty DataFrames.\n",
    "assert not y.empty, \"y is empty.\"\n",
    "\n",
    "# Check for no NaN values in target variable.\n",
    "assert y.isnull().sum().sum() == 0, \"y contains NaN values.\"\n",
    "\n",
    "# Check for correct number of classes.\n",
    "assert len(y[\"y\"].unique()) == 4, \"Incorrect number of classes.\"\n",
    "\n",
    "# Check for matching row numbers.\n",
    "assert features.shape[0] == y.shape[0], \"X and y have different numbers of rows.\"\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Data Split: Train and Test Dataset Creation\n",
    "##################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, y, test_size=0.1, random_state=RS_NUMPY, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Imputation of Missing Values\n",
    "\n",
    "We use the median value estimator derived from the train dataset to impute missing values in both the train and test datasets. This approach is chosen (as opposed to using different estimators per set) because the train dataset is substantially larger and assumed to be \"representative\" of the overall population. Consequently, it provides a more reliable estimator than one that would be calculated from the smaller test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_estimator = X_train.median(axis=0, skipna=True)\n",
    "\n",
    "X_train = X_train.fillna(median_estimator, axis=0)\n",
    "X_test = X_test.fillna(median_estimator, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights for handling class imbalance.\n",
    "sample_weights = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "\n",
    "# Train XGBoost classifier with selected features and sample weights.\n",
    "xgb = XGBClassifier(use_label_encoder=False, n_jobs=-1, random_state=SEED)\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "score_test = f1_score(y_test, y_test_pred, average=\"micro\")\n",
    "print(f\"F1 test set:Â {score_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, WhiteKernel\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\".\")\n",
    "\n",
    "PATH_TO_DATA = ROOT / \"data\"\n",
    "PATH_TO_X = PATH_TO_DATA / \"X_train.csv\"\n",
    "PATH_TO_Y = PATH_TO_DATA / \"y_train.csv\"\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Reproducibility\n",
    "##################################################\n",
    "SEED = 4\n",
    "random.seed(SEED)\n",
    "RS_NUMPY = np.random.RandomState(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Load the data\n",
    "##################################################\n",
    "X = pd.read_csv(PATH_TO_X, index_col=0)\n",
    "y = pd.read_csv(PATH_TO_Y, index_col=0)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Sanity checks\n",
    "##################################################\n",
    "# Check for non-empty DataFrames.\n",
    "assert not X.empty, \"X is empty.\"\n",
    "assert not y.empty, \"y is empty.\"\n",
    "\n",
    "# Check for matching row numbers.\n",
    "assert X.shape[0] == y.shape[0], \"X and y have different numbers of rows.\"\n",
    "\n",
    "# Check for no NaN values in target variable.\n",
    "assert y.isnull().sum().sum() == 0, \"y contains NaN values.\"\n",
    "\n",
    "# Check for non-negative target variable.\n",
    "assert (y >= 0).all().item(), \"y contains negative values.\"\n",
    "\n",
    "\n",
    "# Identify columns with constant values.\n",
    "constant_columns = [col for col in X.columns if X[col].nunique(dropna=True) == 1]\n",
    "\n",
    "print(f\"Constant columns: {constant_columns}\")\n",
    "\n",
    "\n",
    "# Identify duplicated columns.\n",
    "duplicated_columns = X.columns[X.T.duplicated()].to_list()\n",
    "\n",
    "print(f\"Duplicated columns: {duplicated_columns}\")\n",
    "\n",
    "\n",
    "# Drop constant and duplicated columns.\n",
    "columns_to_drop = list(set(constant_columns + duplicated_columns))\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Data Split: Train and Test Dataset Creation\n",
    "##################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=RS_NUMPY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, NaN values are handled through median imputation, as Isolation Forest cannot process NaN values directly.\n",
    "# The median is chosen given that it is robust against outliers.\n",
    "median_estimator = X_train.median(axis=0, skipna=True)\n",
    "\n",
    "X_train_temp = X_train.fillna(median_estimator, axis=0)\n",
    "X_test_temp = X_test.fillna(median_estimator, axis=0)\n",
    "\n",
    "\n",
    "# Using isolation forest to determine outliers.\n",
    "iso = IsolationForest(contamination=0.04, random_state=RS_NUMPY)\n",
    "\n",
    "outliers_pred_train = iso.fit_predict(X_train_temp.to_numpy())\n",
    "non_outliers_mask_train = outliers_pred_train != -1  # Outliers are labeled as -1.\n",
    "\n",
    "outliers_pred_test = iso.predict(X_test_temp.to_numpy())\n",
    "non_outliers_mask_test = outliers_pred_test != -1\n",
    "\n",
    "print(\n",
    "    f\"Detected outliers by Isolation Forest: {np.sum(~non_outliers_mask_train) + np.sum(~non_outliers_mask_test)}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Drop outliers.\n",
    "X_train, y_train = X_train[non_outliers_mask_train], y_train[non_outliers_mask_train]\n",
    "X_test, y_test = X_test[non_outliers_mask_test], y_test[non_outliers_mask_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_coefficients(X, y):\n",
    "    correlations = X.apply(lambda x: x.corr(y))\n",
    "    abs_correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.bar(range(len(abs_correlations)), abs_correlations.values)\n",
    "\n",
    "    # Hide ticks.\n",
    "    plt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Absolute Correlation with Target\")\n",
    "    plt.title(\"Feature Correlations with Target\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # The threshold value is chosen by analysing the plot.\n",
    "    threshold_corr = 0.12\n",
    "    columns_to_drop = abs_correlations[\n",
    "        abs_correlations <= threshold_corr\n",
    "    ].index.to_list()\n",
    "\n",
    "    return columns_to_drop\n",
    "\n",
    "\n",
    "columns_to_drop = plot_correlation_coefficients(X_train, y_train.squeeze())\n",
    "print(f\"Number of features dropped: {len(columns_to_drop)}\")\n",
    "\n",
    "X_train = X_train.drop(columns=columns_to_drop)\n",
    "X_test = X_test.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Imputation of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "# Using k-Nearest Neighbors to compute the imputed values.\n",
    "imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n",
    "\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = C(10.3**2, constant_value_bounds=\"fixed\") * RationalQuadratic(\n",
    "    length_scale=7.25, alpha=0.83, length_scale_bounds=\"fixed\", alpha_bounds=\"fixed\"\n",
    ") + WhiteKernel(noise_level=1.77e-07, noise_level_bounds=\"fixed\")\n",
    "\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=0.01, random_state=RS_NUMPY, n_restarts_optimizer=0\n",
    ")\n",
    "\n",
    "# Fit the GP model.\n",
    "target_mean = y_train.mean().item()\n",
    "gp.fit(X_train, (y_train - target_mean).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_test_pred = gp.predict(X_test) + target_mean\n",
    "\n",
    "score_test = r2_score(y_test, y_test_pred)\n",
    "print(f\"R2 test set: {score_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
